{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_In_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4yvCTrnzS8kFAmHgrSaUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitadhainje/LearningBERT/blob/master/Transformer_In_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewHOMKS-HzpS",
        "colab_type": "text"
      },
      "source": [
        "**TRANSFORMERS**<br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNLwSUs3lTBL",
        "colab_type": "text"
      },
      "source": [
        "**STEP - 1 - IMPORTING REQUIRED LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmcZoKqHHIHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PTG6IkilZ2d",
        "colab_type": "text"
      },
      "source": [
        "**STEP - 2 - IMPORT THE DATA FROM GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpaFIzLjNrx",
        "colab_type": "code",
        "outputId": "f9c22a5a-c211-4d65-b5d8-8b8391382e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05BvnOQHlzM_",
        "colab_type": "code",
        "outputId": "81a1bc92-0c24-4ceb-9295-16ca194ece25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls -ltr /content/drive/'My Drive'/'Colab Notebooks'/NLP/nonbreaking_prefix.fr"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 121 May  1 09:05 '/content/drive/My Drive/Colab Notebooks/NLP/nonbreaking_prefix.fr'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pp6KRVkmJ54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/Colab Notebooks/NLP/nonbreaking_prefix.en\", mode='r', encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/NLP/nonbreaking_prefix.fr\", mode='r', encoding='utf-8') as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yu_lzwjnLjB",
        "colab_type": "code",
        "outputId": "ce88b07e-dcc6-43ac-e098-9f827c085707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (\"Following is the input file in english language - \")\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/NLP/europarl-v7.fr-en.en\", mode='r', encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "europarl_en[0:100]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Following is the input file in english language - \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjD4gqtWpe0g",
        "colab_type": "code",
        "outputId": "5b4b2317-c135-4604-9248-c821b46eff9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (\"Following is the input file in french language - \")\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/NLP/europarl-v7.fr-en.fr\", mode='r', encoding='utf-8') as f:\n",
        "    europarl_fr = f.read()\n",
        "europarl_fr[0:100]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Following is the input file in french language - \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Reprise de la session\\nJe déclare reprise la session du Parlement européen qui avait été interrompue '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RprdPqNUzcAK",
        "colab_type": "text"
      },
      "source": [
        "The above files named \"europarl-v7.fr-en.fr\"  and \"europarl-v7.fr-en.en\" are files containing french and english sentences respectively. We need to convert it into a list of sentences. Hence we will be using the \"nonbreaking\" prefix files for the same. The following step of \"Cleaning the data\" will convert the text in the english and french files into a list of sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOgwliS2s8uQ",
        "colab_type": "text"
      },
      "source": [
        "**STEP - 3 - CLEANING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qONLIRssxoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ENVxwRRtEPk",
        "colab_type": "code",
        "outputId": "9c7712e8-a623-4a3d-b684-718bb75cf3f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "non_breaking_prefix_en"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' a.',\n",
              " ' b.',\n",
              " ' c.',\n",
              " ' d.',\n",
              " ' e.',\n",
              " ' f.',\n",
              " ' g.',\n",
              " ' h.',\n",
              " ' i.',\n",
              " ' j.',\n",
              " ' k.',\n",
              " ' l.',\n",
              " ' m.',\n",
              " ' n.',\n",
              " ' o.',\n",
              " ' p.',\n",
              " ' q.',\n",
              " ' r.',\n",
              " ' s.',\n",
              " ' t.',\n",
              " ' u.',\n",
              " ' v.',\n",
              " ' w.',\n",
              " ' x.',\n",
              " ' y.',\n",
              " ' z.',\n",
              " ' messrs.',\n",
              " ' mlle.',\n",
              " ' mme.',\n",
              " ' mr.',\n",
              " ' mrs.',\n",
              " ' ms.',\n",
              " ' ph.',\n",
              " ' prof.',\n",
              " ' sr.',\n",
              " ' st.',\n",
              " ' a.m.',\n",
              " ' p.m.',\n",
              " ' vs.',\n",
              " ' i.e.',\n",
              " ' e.g.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCYSUjAJtFr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Jl5KgQy49S",
        "colab_type": "code",
        "outputId": "d478a4c1-0189-4321-cb4c-e364d436c503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "print (\"Few english sentences are as follows - \")\n",
        "for x in corpus_en[0:5]:\n",
        "    print (x)\n",
        "print (\"\\n\")\n",
        "print (\"Few french sentences are as follows - \")\n",
        "for x in corpus_fr[0:5]:\n",
        "    print (x)\n",
        "print (\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Few english sentences are as follows - \n",
            "Resumption of the session\n",
            "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
            "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
            "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
            "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
            "\n",
            "\n",
            "Few french sentences are as follows - \n",
            "Reprise de la session\n",
            "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
            "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
            "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\n",
            "En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg0j2Xj6lnKx",
        "colab_type": "text"
      },
      "source": [
        "**STEP - 4 - TOKENIZATION OF DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJqFcbBBlQ-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**9)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_fr, target_vocab_size=2**9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-CmdP5BnSAD",
        "colab_type": "text"
      },
      "source": [
        "We are going to add 2 tokens to our sentences. One at the start of the sentence and another at the end of the sentences. We are going to use this for our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFgE3dSYl93_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1fM42HSnwcl",
        "colab_type": "code",
        "outputId": "874bd580-a463-4fe4-8b15-c80815b8ab66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]\n",
        "print (inputs[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[514, 340, 63, 375, 367, 370, 140, 4, 1, 21, 156, 108, 515]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYjLo4FHobtJ",
        "colab_type": "text"
      },
      "source": [
        "We are removing the sentence with huge size because - \n",
        "\n",
        "\n",
        "*   If we have a huge dataset then the time taken to process it will be also more. Hence for the demo purpose we are removing the sentences which have length greater than 20 words\n",
        "*   Also we are going to pad the sentences and if we consider the length of the longest sequence then their might be more sparse matrices.. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uukvE39an2sU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H0Kz4L1oMdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,value=0,padding='post',maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,value=0,padding='post',maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQhRyUyOz5rQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000 # Further shuffling of datasets\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs)) # all the inputs become a Tensor object\n",
        "\n",
        "dataset = dataset.cache() # to increase the speed of the training\n",
        "\n",
        "# batch: split dataset into subset of the given size.\n",
        "# shuffle: very important function for the training data input pipeline. \n",
        "# But this shuffle requires buffer size that is responsible for the number of elements that will be shuffled.\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# tf.data.experimental.AUTOTUNE defines appropriate number of processes that are free for working.\n",
        "# prefetch: TensorFlow showed very clear picture how to obtain train pipeline in terms of time and memory efficiency.\n",
        "# prefetch doesn’t allow CPU stand idle. When model is training prefetch continue prepare data while GPU is busy.\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUtb9oFk2YJ1",
        "colab_type": "text"
      },
      "source": [
        "**STEP - 5 - BUILD THE TRANSFORMER MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWxWyWOXsh0B",
        "colab_type": "text"
      },
      "source": [
        "The first part of the transformer model are generate the input embedding and positional encoding. We are creating the custom layer for calculating positional embedding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfN04UNC2LmZ",
        "colab_type": "text"
      },
      "source": [
        "**POSTIONAL ENCODING**<br />\n",
        "The order of the sequence provides important information for machine translation and language modelling tasks. Hence we add positional information of the input tokens in the sequence to the input embedding vectors. This layer will only perform mathematical calculations.\n",
        "\n",
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$\n",
        "\n",
        "The above formulae are mentioned in the Transfomer research paper.\n",
        "<br />For information on Positional Encoding - https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ <br />\n",
        "https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56XFFmDN2GBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomtVlUx2sUa",
        "colab_type": "text"
      },
      "source": [
        "**CALCULATE THE ATTENTION** <br />\n",
        "Attention is described as a function of query and a set of key-value pairs. General structure of the scaled dot product takes input as queries, keys and values. Queries, keys and values , each represent a sentence. Here \"mask\" can be of two types - \n",
        "*   The \"look ahead\" mask which does not allow the decoder to check the previous word OR\n",
        "*   The padding mask in the sentence i.e. the \"zeros\" as the end of each sentence to get all the sentences at same size\n",
        "<br />The scaled dot product attention function is applied multiple times on the inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHFlQVQs2rt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    # transpose_b = means that the second matrix will be transposed.\n",
        "    \n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NNNtqOr26DN",
        "colab_type": "text"
      },
      "source": [
        "**MULTI-HEAD ATTENTION** <br />\n",
        "According to the architecture mentioned in the research paper, for each of the inputs (keys, values and queries), we will apply a dense layer (i.e. a linear function). Then we split it into sub-spaces and then apply scaled-dot product attention. Finally we will apply concatenation and then a linear computation to make it digestible to the next layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRGaglRm2xh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    # init is called when the object is created\n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "    \n",
        "    # build is called when the object is used\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x2-9lKW29OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2uckaLypgr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgV10lSnphi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw66Zqdbp9sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlGdwpgKp-Qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xs5PYnFqENU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MVMmD5oqHiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xisOujRKqJar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsN_3--AqMLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/Colab Notebooks/NLP/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI0c9-Eyqj-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4bf8a94-010c-4f4a-9c93-496b8be90ad2"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 4.5270 Accuracy 0.0016\n",
            "Epoch 1 Batch 50 Loss 4.5323 Accuracy 0.0047\n",
            "Epoch 1 Batch 100 Loss 4.3617 Accuracy 0.0267\n",
            "Epoch 1 Batch 150 Loss 4.2363 Accuracy 0.0363\n",
            "Epoch 1 Batch 200 Loss 4.1165 Accuracy 0.0474\n",
            "Epoch 1 Batch 250 Loss 3.9901 Accuracy 0.0597\n",
            "Epoch 1 Batch 300 Loss 3.8672 Accuracy 0.0735\n",
            "Epoch 1 Batch 350 Loss 3.7406 Accuracy 0.0896\n",
            "Epoch 1 Batch 400 Loss 3.6268 Accuracy 0.1045\n",
            "Epoch 1 Batch 450 Loss 3.5210 Accuracy 0.1181\n",
            "Epoch 1 Batch 500 Loss 3.4196 Accuracy 0.1309\n",
            "Epoch 1 Batch 550 Loss 3.3224 Accuracy 0.1428\n",
            "Epoch 1 Batch 600 Loss 3.2347 Accuracy 0.1535\n",
            "Epoch 1 Batch 650 Loss 3.1536 Accuracy 0.1640\n",
            "Epoch 1 Batch 700 Loss 3.0814 Accuracy 0.1737\n",
            "Epoch 1 Batch 750 Loss 3.0174 Accuracy 0.1826\n",
            "Epoch 1 Batch 800 Loss 2.9523 Accuracy 0.1917\n",
            "Epoch 1 Batch 850 Loss 2.8942 Accuracy 0.2002\n",
            "Epoch 1 Batch 900 Loss 2.8373 Accuracy 0.2083\n",
            "Epoch 1 Batch 950 Loss 2.7839 Accuracy 0.2161\n",
            "Epoch 1 Batch 1000 Loss 2.7331 Accuracy 0.2231\n",
            "Epoch 1 Batch 1050 Loss 2.6834 Accuracy 0.2298\n",
            "Epoch 1 Batch 1100 Loss 2.6386 Accuracy 0.2361\n",
            "Epoch 1 Batch 1150 Loss 2.5943 Accuracy 0.2422\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-1\n",
            "Time taken for 1 epoch: 989.2967350482941 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.7900 Accuracy 0.3363\n",
            "Epoch 2 Batch 50 Loss 1.6577 Accuracy 0.3753\n",
            "Epoch 2 Batch 100 Loss 1.6365 Accuracy 0.3771\n",
            "Epoch 2 Batch 150 Loss 1.6149 Accuracy 0.3784\n",
            "Epoch 2 Batch 200 Loss 1.5870 Accuracy 0.3805\n",
            "Epoch 2 Batch 250 Loss 1.5614 Accuracy 0.3825\n",
            "Epoch 2 Batch 300 Loss 1.5333 Accuracy 0.3853\n",
            "Epoch 2 Batch 350 Loss 1.5091 Accuracy 0.3873\n",
            "Epoch 2 Batch 400 Loss 1.4902 Accuracy 0.3897\n",
            "Epoch 2 Batch 450 Loss 1.4684 Accuracy 0.3919\n",
            "Epoch 2 Batch 500 Loss 1.4475 Accuracy 0.3941\n",
            "Epoch 2 Batch 550 Loss 1.4321 Accuracy 0.3956\n",
            "Epoch 2 Batch 600 Loss 1.4207 Accuracy 0.3974\n",
            "Epoch 2 Batch 650 Loss 1.4076 Accuracy 0.3987\n",
            "Epoch 2 Batch 700 Loss 1.3996 Accuracy 0.4004\n",
            "Epoch 2 Batch 750 Loss 1.3939 Accuracy 0.4017\n",
            "Epoch 2 Batch 800 Loss 1.3881 Accuracy 0.4030\n",
            "Epoch 2 Batch 850 Loss 1.3820 Accuracy 0.4048\n",
            "Epoch 2 Batch 900 Loss 1.3763 Accuracy 0.4065\n",
            "Epoch 2 Batch 950 Loss 1.3698 Accuracy 0.4079\n",
            "Epoch 2 Batch 1000 Loss 1.3642 Accuracy 0.4092\n",
            "Epoch 2 Batch 1050 Loss 1.3575 Accuracy 0.4105\n",
            "Epoch 2 Batch 1100 Loss 1.3529 Accuracy 0.4117\n",
            "Epoch 2 Batch 1150 Loss 1.3455 Accuracy 0.4130\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-2\n",
            "Time taken for 1 epoch: 997.0750131607056 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.4383 Accuracy 0.4433\n",
            "Epoch 3 Batch 50 Loss 1.2495 Accuracy 0.4393\n",
            "Epoch 3 Batch 100 Loss 1.2522 Accuracy 0.4407\n",
            "Epoch 3 Batch 150 Loss 1.2382 Accuracy 0.4393\n",
            "Epoch 3 Batch 200 Loss 1.2136 Accuracy 0.4406\n",
            "Epoch 3 Batch 250 Loss 1.1968 Accuracy 0.4413\n",
            "Epoch 3 Batch 300 Loss 1.1786 Accuracy 0.4410\n",
            "Epoch 3 Batch 350 Loss 1.1659 Accuracy 0.4421\n",
            "Epoch 3 Batch 400 Loss 1.1533 Accuracy 0.4426\n",
            "Epoch 3 Batch 450 Loss 1.1377 Accuracy 0.4433\n",
            "Epoch 3 Batch 500 Loss 1.1305 Accuracy 0.4445\n",
            "Epoch 3 Batch 550 Loss 1.1212 Accuracy 0.4458\n",
            "Epoch 3 Batch 600 Loss 1.1094 Accuracy 0.4467\n",
            "Epoch 3 Batch 650 Loss 1.1049 Accuracy 0.4478\n",
            "Epoch 3 Batch 700 Loss 1.1019 Accuracy 0.4487\n",
            "Epoch 3 Batch 750 Loss 1.0998 Accuracy 0.4496\n",
            "Epoch 3 Batch 800 Loss 1.0989 Accuracy 0.4504\n",
            "Epoch 3 Batch 850 Loss 1.0972 Accuracy 0.4514\n",
            "Epoch 3 Batch 900 Loss 1.0981 Accuracy 0.4522\n",
            "Epoch 3 Batch 950 Loss 1.0966 Accuracy 0.4531\n",
            "Epoch 3 Batch 1000 Loss 1.0930 Accuracy 0.4543\n",
            "Epoch 3 Batch 1050 Loss 1.0927 Accuracy 0.4551\n",
            "Epoch 3 Batch 1100 Loss 1.0910 Accuracy 0.4558\n",
            "Epoch 3 Batch 1150 Loss 1.0898 Accuracy 0.4564\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-3\n",
            "Time taken for 1 epoch: 1009.1299738883972 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.0393 Accuracy 0.4753\n",
            "Epoch 4 Batch 50 Loss 1.0500 Accuracy 0.4700\n",
            "Epoch 4 Batch 100 Loss 1.0642 Accuracy 0.4667\n",
            "Epoch 4 Batch 150 Loss 1.0597 Accuracy 0.4691\n",
            "Epoch 4 Batch 200 Loss 1.0511 Accuracy 0.4685\n",
            "Epoch 4 Batch 250 Loss 1.0344 Accuracy 0.4698\n",
            "Epoch 4 Batch 300 Loss 1.0221 Accuracy 0.4701\n",
            "Epoch 4 Batch 350 Loss 1.0086 Accuracy 0.4720\n",
            "Epoch 4 Batch 400 Loss 0.9948 Accuracy 0.4726\n",
            "Epoch 4 Batch 450 Loss 0.9885 Accuracy 0.4733\n",
            "Epoch 4 Batch 500 Loss 0.9826 Accuracy 0.4729\n",
            "Epoch 4 Batch 550 Loss 0.9755 Accuracy 0.4734\n",
            "Epoch 4 Batch 600 Loss 0.9671 Accuracy 0.4740\n",
            "Epoch 4 Batch 650 Loss 0.9651 Accuracy 0.4745\n",
            "Epoch 4 Batch 700 Loss 0.9631 Accuracy 0.4752\n",
            "Epoch 4 Batch 750 Loss 0.9642 Accuracy 0.4756\n",
            "Epoch 4 Batch 800 Loss 0.9654 Accuracy 0.4759\n",
            "Epoch 4 Batch 850 Loss 0.9646 Accuracy 0.4764\n",
            "Epoch 4 Batch 900 Loss 0.9646 Accuracy 0.4772\n",
            "Epoch 4 Batch 950 Loss 0.9622 Accuracy 0.4781\n",
            "Epoch 4 Batch 1000 Loss 0.9621 Accuracy 0.4790\n",
            "Epoch 4 Batch 1050 Loss 0.9622 Accuracy 0.4796\n",
            "Epoch 4 Batch 1100 Loss 0.9621 Accuracy 0.4803\n",
            "Epoch 4 Batch 1150 Loss 0.9597 Accuracy 0.4810\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 995.2167456150055 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 0.8245 Accuracy 0.5288\n",
            "Epoch 5 Batch 50 Loss 0.9406 Accuracy 0.4884\n",
            "Epoch 5 Batch 100 Loss 0.9472 Accuracy 0.4911\n",
            "Epoch 5 Batch 150 Loss 0.9299 Accuracy 0.4920\n",
            "Epoch 5 Batch 200 Loss 0.9185 Accuracy 0.4921\n",
            "Epoch 5 Batch 250 Loss 0.9095 Accuracy 0.4923\n",
            "Epoch 5 Batch 300 Loss 0.8994 Accuracy 0.4936\n",
            "Epoch 5 Batch 350 Loss 0.8910 Accuracy 0.4943\n",
            "Epoch 5 Batch 400 Loss 0.8793 Accuracy 0.4951\n",
            "Epoch 5 Batch 450 Loss 0.8671 Accuracy 0.4963\n",
            "Epoch 5 Batch 500 Loss 0.8579 Accuracy 0.4968\n",
            "Epoch 5 Batch 550 Loss 0.8523 Accuracy 0.4975\n",
            "Epoch 5 Batch 600 Loss 0.8466 Accuracy 0.4981\n",
            "Epoch 5 Batch 650 Loss 0.8443 Accuracy 0.4986\n",
            "Epoch 5 Batch 700 Loss 0.8441 Accuracy 0.4990\n",
            "Epoch 5 Batch 750 Loss 0.8446 Accuracy 0.4998\n",
            "Epoch 5 Batch 800 Loss 0.8451 Accuracy 0.5004\n",
            "Epoch 5 Batch 850 Loss 0.8459 Accuracy 0.5010\n",
            "Epoch 5 Batch 900 Loss 0.8476 Accuracy 0.5015\n",
            "Epoch 5 Batch 950 Loss 0.8482 Accuracy 0.5023\n",
            "Epoch 5 Batch 1000 Loss 0.8470 Accuracy 0.5029\n",
            "Epoch 5 Batch 1050 Loss 0.8469 Accuracy 0.5036\n",
            "Epoch 5 Batch 1100 Loss 0.8461 Accuracy 0.5044\n",
            "Epoch 5 Batch 1150 Loss 0.8443 Accuracy 0.5050\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 986.1625304222107 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 0.8647 Accuracy 0.5206\n",
            "Epoch 6 Batch 50 Loss 0.8403 Accuracy 0.5076\n",
            "Epoch 6 Batch 100 Loss 0.8387 Accuracy 0.5117\n",
            "Epoch 6 Batch 150 Loss 0.8288 Accuracy 0.5127\n",
            "Epoch 6 Batch 200 Loss 0.8198 Accuracy 0.5142\n",
            "Epoch 6 Batch 250 Loss 0.8151 Accuracy 0.5143\n",
            "Epoch 6 Batch 300 Loss 0.8045 Accuracy 0.5155\n",
            "Epoch 6 Batch 350 Loss 0.7941 Accuracy 0.5159\n",
            "Epoch 6 Batch 400 Loss 0.7839 Accuracy 0.5160\n",
            "Epoch 6 Batch 450 Loss 0.7774 Accuracy 0.5163\n",
            "Epoch 6 Batch 500 Loss 0.7683 Accuracy 0.5166\n",
            "Epoch 6 Batch 550 Loss 0.7635 Accuracy 0.5166\n",
            "Epoch 6 Batch 600 Loss 0.7596 Accuracy 0.5168\n",
            "Epoch 6 Batch 650 Loss 0.7579 Accuracy 0.5171\n",
            "Epoch 6 Batch 700 Loss 0.7583 Accuracy 0.5178\n",
            "Epoch 6 Batch 750 Loss 0.7589 Accuracy 0.5178\n",
            "Epoch 6 Batch 800 Loss 0.7601 Accuracy 0.5183\n",
            "Epoch 6 Batch 850 Loss 0.7603 Accuracy 0.5185\n",
            "Epoch 6 Batch 900 Loss 0.7614 Accuracy 0.5190\n",
            "Epoch 6 Batch 950 Loss 0.7626 Accuracy 0.5199\n",
            "Epoch 6 Batch 1000 Loss 0.7624 Accuracy 0.5202\n",
            "Epoch 6 Batch 1050 Loss 0.7624 Accuracy 0.5206\n",
            "Epoch 6 Batch 1100 Loss 0.7631 Accuracy 0.5212\n",
            "Epoch 6 Batch 1150 Loss 0.7634 Accuracy 0.5218\n",
            "Saving checkpoint for epoch 6 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-6\n",
            "Time taken for 1 epoch: 1004.2406430244446 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 0.7456 Accuracy 0.5296\n",
            "Epoch 7 Batch 50 Loss 0.7705 Accuracy 0.5278\n",
            "Epoch 7 Batch 100 Loss 0.7695 Accuracy 0.5295\n",
            "Epoch 7 Batch 150 Loss 0.7596 Accuracy 0.5292\n",
            "Epoch 7 Batch 200 Loss 0.7551 Accuracy 0.5286\n",
            "Epoch 7 Batch 250 Loss 0.7447 Accuracy 0.5282\n",
            "Epoch 7 Batch 300 Loss 0.7364 Accuracy 0.5292\n",
            "Epoch 7 Batch 350 Loss 0.7312 Accuracy 0.5291\n",
            "Epoch 7 Batch 400 Loss 0.7243 Accuracy 0.5291\n",
            "Epoch 7 Batch 450 Loss 0.7211 Accuracy 0.5292\n",
            "Epoch 7 Batch 500 Loss 0.7141 Accuracy 0.5292\n",
            "Epoch 7 Batch 550 Loss 0.7074 Accuracy 0.5295\n",
            "Epoch 7 Batch 600 Loss 0.7014 Accuracy 0.5293\n",
            "Epoch 7 Batch 650 Loss 0.6997 Accuracy 0.5293\n",
            "Epoch 7 Batch 700 Loss 0.6993 Accuracy 0.5290\n",
            "Epoch 7 Batch 750 Loss 0.6997 Accuracy 0.5295\n",
            "Epoch 7 Batch 800 Loss 0.7029 Accuracy 0.5302\n",
            "Epoch 7 Batch 850 Loss 0.7033 Accuracy 0.5308\n",
            "Epoch 7 Batch 900 Loss 0.7049 Accuracy 0.5311\n",
            "Epoch 7 Batch 950 Loss 0.7050 Accuracy 0.5317\n",
            "Epoch 7 Batch 1000 Loss 0.7056 Accuracy 0.5321\n",
            "Epoch 7 Batch 1050 Loss 0.7065 Accuracy 0.5324\n",
            "Epoch 7 Batch 1100 Loss 0.7061 Accuracy 0.5326\n",
            "Epoch 7 Batch 1150 Loss 0.7074 Accuracy 0.5331\n",
            "Saving checkpoint for epoch 7 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-7\n",
            "Time taken for 1 epoch: 995.3772180080414 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 0.6413 Accuracy 0.5066\n",
            "Epoch 8 Batch 50 Loss 0.7301 Accuracy 0.5442\n",
            "Epoch 8 Batch 100 Loss 0.7216 Accuracy 0.5416\n",
            "Epoch 8 Batch 150 Loss 0.7187 Accuracy 0.5417\n",
            "Epoch 8 Batch 200 Loss 0.7129 Accuracy 0.5403\n",
            "Epoch 8 Batch 250 Loss 0.7066 Accuracy 0.5391\n",
            "Epoch 8 Batch 300 Loss 0.6968 Accuracy 0.5386\n",
            "Epoch 8 Batch 350 Loss 0.6861 Accuracy 0.5378\n",
            "Epoch 8 Batch 400 Loss 0.6779 Accuracy 0.5383\n",
            "Epoch 8 Batch 450 Loss 0.6696 Accuracy 0.5388\n",
            "Epoch 8 Batch 500 Loss 0.6647 Accuracy 0.5392\n",
            "Epoch 8 Batch 550 Loss 0.6609 Accuracy 0.5391\n",
            "Epoch 8 Batch 600 Loss 0.6579 Accuracy 0.5392\n",
            "Epoch 8 Batch 650 Loss 0.6560 Accuracy 0.5390\n",
            "Epoch 8 Batch 700 Loss 0.6553 Accuracy 0.5393\n",
            "Epoch 8 Batch 750 Loss 0.6577 Accuracy 0.5396\n",
            "Epoch 8 Batch 800 Loss 0.6599 Accuracy 0.5400\n",
            "Epoch 8 Batch 850 Loss 0.6608 Accuracy 0.5404\n",
            "Epoch 8 Batch 900 Loss 0.6609 Accuracy 0.5407\n",
            "Epoch 8 Batch 950 Loss 0.6617 Accuracy 0.5414\n",
            "Epoch 8 Batch 1000 Loss 0.6632 Accuracy 0.5416\n",
            "Epoch 8 Batch 1050 Loss 0.6631 Accuracy 0.5422\n",
            "Epoch 8 Batch 1100 Loss 0.6639 Accuracy 0.5425\n",
            "Epoch 8 Batch 1150 Loss 0.6638 Accuracy 0.5426\n",
            "Saving checkpoint for epoch 8 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-8\n",
            "Time taken for 1 epoch: 984.8862376213074 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 0.8019 Accuracy 0.5271\n",
            "Epoch 9 Batch 50 Loss 0.6807 Accuracy 0.5525\n",
            "Epoch 9 Batch 100 Loss 0.6785 Accuracy 0.5505\n",
            "Epoch 9 Batch 150 Loss 0.6787 Accuracy 0.5489\n",
            "Epoch 9 Batch 200 Loss 0.6719 Accuracy 0.5460\n",
            "Epoch 9 Batch 250 Loss 0.6611 Accuracy 0.5451\n",
            "Epoch 9 Batch 300 Loss 0.6563 Accuracy 0.5453\n",
            "Epoch 9 Batch 350 Loss 0.6489 Accuracy 0.5461\n",
            "Epoch 9 Batch 400 Loss 0.6410 Accuracy 0.5460\n",
            "Epoch 9 Batch 450 Loss 0.6356 Accuracy 0.5458\n",
            "Epoch 9 Batch 500 Loss 0.6306 Accuracy 0.5459\n",
            "Epoch 9 Batch 550 Loss 0.6257 Accuracy 0.5452\n",
            "Epoch 9 Batch 600 Loss 0.6209 Accuracy 0.5451\n",
            "Epoch 9 Batch 650 Loss 0.6194 Accuracy 0.5456\n",
            "Epoch 9 Batch 700 Loss 0.6195 Accuracy 0.5459\n",
            "Epoch 9 Batch 750 Loss 0.6212 Accuracy 0.5461\n",
            "Epoch 9 Batch 800 Loss 0.6237 Accuracy 0.5468\n",
            "Epoch 9 Batch 850 Loss 0.6254 Accuracy 0.5473\n",
            "Epoch 9 Batch 900 Loss 0.6255 Accuracy 0.5477\n",
            "Epoch 9 Batch 950 Loss 0.6260 Accuracy 0.5485\n",
            "Epoch 9 Batch 1000 Loss 0.6270 Accuracy 0.5490\n",
            "Epoch 9 Batch 1050 Loss 0.6284 Accuracy 0.5493\n",
            "Epoch 9 Batch 1100 Loss 0.6291 Accuracy 0.5497\n",
            "Epoch 9 Batch 1150 Loss 0.6301 Accuracy 0.5498\n",
            "Saving checkpoint for epoch 9 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-9\n",
            "Time taken for 1 epoch: 990.1125028133392 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 0.7909 Accuracy 0.5304\n",
            "Epoch 10 Batch 50 Loss 0.6465 Accuracy 0.5502\n",
            "Epoch 10 Batch 100 Loss 0.6427 Accuracy 0.5550\n",
            "Epoch 10 Batch 150 Loss 0.6435 Accuracy 0.5552\n",
            "Epoch 10 Batch 200 Loss 0.6373 Accuracy 0.5554\n",
            "Epoch 10 Batch 250 Loss 0.6283 Accuracy 0.5543\n",
            "Epoch 10 Batch 300 Loss 0.6210 Accuracy 0.5538\n",
            "Epoch 10 Batch 350 Loss 0.6129 Accuracy 0.5534\n",
            "Epoch 10 Batch 400 Loss 0.6047 Accuracy 0.5530\n",
            "Epoch 10 Batch 450 Loss 0.5997 Accuracy 0.5524\n",
            "Epoch 10 Batch 500 Loss 0.5948 Accuracy 0.5519\n",
            "Epoch 10 Batch 550 Loss 0.5900 Accuracy 0.5519\n",
            "Epoch 10 Batch 600 Loss 0.5875 Accuracy 0.5518\n",
            "Epoch 10 Batch 650 Loss 0.5870 Accuracy 0.5519\n",
            "Epoch 10 Batch 700 Loss 0.5869 Accuracy 0.5520\n",
            "Epoch 10 Batch 750 Loss 0.5896 Accuracy 0.5522\n",
            "Epoch 10 Batch 800 Loss 0.5914 Accuracy 0.5525\n",
            "Epoch 10 Batch 850 Loss 0.5936 Accuracy 0.5534\n",
            "Epoch 10 Batch 900 Loss 0.5954 Accuracy 0.5539\n",
            "Epoch 10 Batch 950 Loss 0.5971 Accuracy 0.5542\n",
            "Epoch 10 Batch 1000 Loss 0.5992 Accuracy 0.5544\n",
            "Epoch 10 Batch 1050 Loss 0.6000 Accuracy 0.5552\n",
            "Epoch 10 Batch 1100 Loss 0.6003 Accuracy 0.5555\n",
            "Epoch 10 Batch 1150 Loss 0.6010 Accuracy 0.5558\n",
            "Saving checkpoint for epoch 10 at ./drive/My Drive/Colab Notebooks/NLP/ckpt/ckpt-10\n",
            "Time taken for 1 epoch: 1004.7171549797058 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvU1qt-YqnQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdhwh5POqpRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGyCdCN6qsSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a010dd62-1bb4-4524-a372-a1e8fbc946d4"
      },
      "source": [
        "translate(\"The session was interesting\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: The session was interesting\n",
            "Predicted translation: La session était intéressante\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5dpfjlTp6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}