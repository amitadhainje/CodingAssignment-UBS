{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_In_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBJcYSSfhYbvMe6njlNyNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitadhainje/LearningBERT/blob/master/Transformer_In_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewHOMKS-HzpS",
        "colab_type": "text"
      },
      "source": [
        "**TRANSFORMERS**<br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNLwSUs3lTBL",
        "colab_type": "text"
      },
      "source": [
        "**STEP - 1 - IMPORTING REQUIRED LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmcZoKqHHIHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PTG6IkilZ2d",
        "colab_type": "text"
      },
      "source": [
        "**STEP - 2 - IMPORT THE DATA FROM GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpaFIzLjNrx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "93e2d53a-8f7f-474e-a056-10da4b9af60f"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05BvnOQHlzM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73d74e95-e212-498a-c653-2d47dd463391"
      },
      "source": [
        "!ls -ltr /content/drive/'My Drive'/'Colab Notebooks'/NLP/nonbreaking_prefix.fr"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 121 May  1 09:05 '/content/drive/My Drive/Colab Notebooks/NLP/nonbreaking_prefix.fr'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pp6KRVkmJ54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/Colab Notebooks/NLP/nonbreaking_prefix.en\", mode='r', encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/NLP/nonbreaking_prefix.fr\", mode='r', encoding='utf-8') as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yu_lzwjnLjB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "38b4c17a-aab3-4768-8763-0f975631ea92"
      },
      "source": [
        "print (\"Following is the input file in english language - \")\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/NLP/europarl-v7.fr-en.en\", mode='r', encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "europarl_en[0:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Following is the input file in english language - \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjD4gqtWpe0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fe6c50c3-4492-4853-c69f-6a0d7fbb10ec"
      },
      "source": [
        "print (\"Following is the input file in french language - \")\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/NLP/europarl-v7.fr-en.fr\", mode='r', encoding='utf-8') as f:\n",
        "    europarl_fr = f.read()\n",
        "europarl_fr[0:100]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Following is the input file in french language - \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Reprise de la session\\nJe déclare reprise la session du Parlement européen qui avait été interrompue '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RprdPqNUzcAK",
        "colab_type": "text"
      },
      "source": [
        "The above files named \"europarl-v7.fr-en.fr\"  and \"europarl-v7.fr-en.en\" are files containing french and english sentences respectively. We need to convert it into a list of sentences. Hence we will be using the \"nonbreaking\" prefix files for the same. The following step of \"Cleaning the data\" will convert the text in the english and french files into a list of sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOgwliS2s8uQ",
        "colab_type": "text"
      },
      "source": [
        "**STEP - 3 - CLEANING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qONLIRssxoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ENVxwRRtEPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "b4436be9-80c7-4c7c-f728-5bd72cba76e2"
      },
      "source": [
        "non_breaking_prefix_en"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' a.',\n",
              " ' b.',\n",
              " ' c.',\n",
              " ' d.',\n",
              " ' e.',\n",
              " ' f.',\n",
              " ' g.',\n",
              " ' h.',\n",
              " ' i.',\n",
              " ' j.',\n",
              " ' k.',\n",
              " ' l.',\n",
              " ' m.',\n",
              " ' n.',\n",
              " ' o.',\n",
              " ' p.',\n",
              " ' q.',\n",
              " ' r.',\n",
              " ' s.',\n",
              " ' t.',\n",
              " ' u.',\n",
              " ' v.',\n",
              " ' w.',\n",
              " ' x.',\n",
              " ' y.',\n",
              " ' z.',\n",
              " ' messrs.',\n",
              " ' mlle.',\n",
              " ' mme.',\n",
              " ' mr.',\n",
              " ' mrs.',\n",
              " ' ms.',\n",
              " ' ph.',\n",
              " ' prof.',\n",
              " ' sr.',\n",
              " ' st.',\n",
              " ' a.m.',\n",
              " ' p.m.',\n",
              " ' vs.',\n",
              " ' i.e.',\n",
              " ' e.g.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCYSUjAJtFr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Jl5KgQy49S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "00ac15cd-7f07-4118-cc8e-09f510c75272"
      },
      "source": [
        "print (\"Few english sentences are as follows - \")\n",
        "for x in corpus_en[0:5]:\n",
        "    print (x)\n",
        "print (\"\\n\")\n",
        "print (\"Few french sentences are as follows - \")\n",
        "for x in corpus_fr[0:5]:\n",
        "    print (x)\n",
        "print (\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Few english sentences are as follows - \n",
            "Resumption of the session\n",
            "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
            "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
            "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
            "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
            "\n",
            "\n",
            "Few french sentences are as follows - \n",
            "Reprise de la session\n",
            "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
            "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
            "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\n",
            "En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg0j2Xj6lnKx",
        "colab_type": "text"
      },
      "source": [
        "**STEP - 4 - TOKENIZATION OF DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJqFcbBBlQ-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**9)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_fr, target_vocab_size=2**9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-CmdP5BnSAD",
        "colab_type": "text"
      },
      "source": [
        "We are going to add 2 tokens to our sentences. One at the start of the sentence and another at the end of the sentences. We are going to use this for our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFgE3dSYl93_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1fM42HSnwcl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8800dea-bbd1-42a3-f80d-aaac469f3d02"
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]\n",
        "print (inputs[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[514, 340, 63, 375, 367, 370, 140, 4, 1, 21, 156, 108, 515]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYjLo4FHobtJ",
        "colab_type": "text"
      },
      "source": [
        "We are removing the sentence with huge size because - \n",
        "\n",
        "\n",
        "*   If we have a huge dataset then the time taken to process it will be also more. Hence for the demo purpose we are removing the sentences which have length greater than 20 words\n",
        "*   Also we are going to pad the sentences and if we consider the length of the longest sequence then their might be more sparse matrices.. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uukvE39an2sU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H0Kz4L1oMdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}